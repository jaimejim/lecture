<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title></title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body class="light">
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { } 
            if (theme === null || theme === undefined) { theme = default_theme; }
            document.body.className = theme;
            document.querySelector('html').className = theme + ' js';
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <ol class="chapter"><li><a href="introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li><a href="ietf.html"><strong aria-hidden="true">2.</strong> The IETF</a></li><li><ol class="section"><li><a href="10years.html"><strong aria-hidden="true">2.1.</strong> A decade of Internet Evolution</a></li><li><a href="another10years.html"><strong aria-hidden="true">2.2.</strong> Another 10 years</a></li></ol></li><li><a href="coapddos.html"><strong aria-hidden="true">3.</strong> DDOS using CoAP</a></li></ol>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar" class="menu-bar">
                    <div id="menu-bar-sticky-container">
                        <div class="left-buttons">
                            <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                                <i class="fa fa-bars"></i>
                            </button>
                            <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                                <i class="fa fa-paint-brush"></i>
                            </button>
                            <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                                <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            </ul>
                            
                            <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                                <i class="fa fa-search"></i>
                            </button>
                            
                        </div>

                        <h1 class="menu-title"></h1> 

                        <div class="right-buttons">
                            <a href="print.html" title="Print this book" aria-label="Print this book">
                                <i id="print-button" class="fa fa-print"></i>
                            </a>
                            
                        </div>
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <a class="header" href="#lecture-notes" id="lecture-notes"><h1>Lecture Notes</h1></a>
<p>Here is a compilation of notes and potential lecture materials around various topics I have condensed over the course of <code>1 month</code>.</p>
<p>The notes are written in markdown and generated using <a href="https://github.com/rust-lang-nursery/mdBook">mdBook</a>.</p>
<p>The contents are <a href="https://github.com/jaimejim/lecture/blob/master/LICENSE">MIT Licensed</a> and so is any code they might contain. You can find the source code on <a href="https://github.com/jaimejim/lecture">GitHub</a>. Issues and feature requests can be posted on the <a href="https://github.com/jaimejim/lecture/issues">GitHub issue tracker</a>.</p>
<a class="header" href="#license" id="license"><h1>License</h1></a>
<p>All the source code is released under the <a href="https://opensource.org/licenses/MIT">MIT Licensed</a>.</p>
<a class="header" href="#the-internet-engineering-task-force-ietf" id="the-internet-engineering-task-force-ietf"><h1>The Internet Engineering Task Force (IETF)</h1></a>
<p>In 1998 the Internet had about 50 million users, supported by approximately 25 million servers (Web and e-mail hosting sites, for example, but not desktops or laptops). Today the estimation is that some 3.4 billion people are regular users of the Internet, and there are some 20 billion devices connected to it. We have achieved this using some 3 billion unique IPv4 addresses. Nobody thought that we could achieve this astonishing feat, yet it has happened with almost no fanfare.</p>
<p>A lot of this success has been done by the work of engineers at organizations that are more or less unknown to the public. Being the Internet Engineering Task Force (IETF) the most prominent.</p>
<p>##Â What has the IETF done for us anyways?</p>
<p>The IETF is a large open international community of network designers, operators, vendors, and researchers concerned with the evolution of the Internet architecture and the smooth operation of the Internet. The mission of the IETF is to make the Internet work better by producing high quality, relevant technical documents that influence the way people design, use, and manage the Internet. <a href="https://tools.ietf.org/html/rfc3935">RFC 3935</a>. Many of those RFCs have already been implemented on every connected machine today:</p>
<ul>
<li><a href="https://tools.ietf.org/html/rfc791">RFC791</a> The Internet Protocol.</li>
<li><a href="https://tools.ietf.org/html/rfc792">RFC792</a> The Internet Control Message Protocol.</li>
<li><a href="https://tools.ietf.org/html/rfc821">RFC821</a> The Simple Mail Transfer Protocol.</li>
<li><a href="https://tools.ietf.org/html/rfc768">RFC768</a> User Datagram Protocol.</li>
<li><a href="https://tools.ietf.org/html/rfc959">RFC959</a> The File Transfer Protocol.</li>
<li><a href="https://tools.ietf.org/html/rfc793">RFC793</a> The Transmission Control Protocol.</li>
<li><a href="https://tools.ietf.org/html/rfc854">RFC854</a> Telnet Specification.</li>
<li><a href="https://tools.ietf.org/html/rfc1119">RFC1119</a> Network Time Protocol.</li>
<li><a href="https://tools.ietf.org/html/rfc1157">RFC1157</a> A Simple Network Management Protocol.</li>
<li><a href="https://tools.ietf.org/html/rfc1035">RFC1035</a> Domain names - implementation and specification.</li>
<li><a href="https://tools.ietf.org/html/rfc1945">RFC1945</a> Hypertext Transfer Protocol.</li>
<li><a href="https://tools.ietf.org/html/rfc1964">RFC1964</a> The Kerberos Version 5 GSS-API Mechanism.</li>
<li><a href="https://tools.ietf.org/html/rfc2131">RFC2131</a> Dynamic Host Configuration Protocol.</li>
<li><a href="https://tools.ietf.org/html/rfc2246">RFC2246</a> The TLS Protocol Version.</li>
<li><a href="https://tools.ietf.org/html/rfc2328">RFC2328</a> The Open Shortest Path First routing protocol.</li>
<li><a href="https://tools.ietf.org/html/rfc6455">RFC6455</a> The WebSocket Protocol.</li>
<li><a href="https://tools.ietf.org/html/rfc5321">RFC5321</a> Simple Mail Transfer Protocol.</li>
<li><a href="https://tools.ietf.org/html/rfc7540">RFC7540</a> Hypertext Transfer Protocol Version 2.</li>
<li><a href="https://tools.ietf.org/html/rfc6749">RFC6749</a> The OAuth 2.0 Authorization Framework.</li>
<li><a href="https://tools.ietf.org/html/rfc4271">RFC4271</a> The Border Gateway Protocol.</li>
<li><a href="https://tools.ietf.org/html/rfc4287">RFC4287</a> The Atom Syndication Format.</li>
<li><a href="https://tools.ietf.org/html/rfc4251">RFC4251</a> The Secure Shell (SSH) Protocol Architecture.</li>
<li><a href="https://tools.ietf.org/html/rfc8200">RFC8200</a> Internet Protocol, Version 6 (IPv6) Sepcification.</li>
</ul>
<p>While most RFCs are just proposals, miscellaneous protocol descriptions, or opinion/information documents, there are true <em>âInternet standardsâ</em> among the RFCs and are maintained by the <a href="http://www.rfc-editor.org/standards">RFC Editor</a>.</p>
<a class="header" href="#ietf-organization" id="ietf-organization"><h2>IETF Organization</h2></a>
<p>As explained at the <a href="https://www.ietf.org/about/who/">IETF website</a>, the technical work of the IETF is done in its <a href="https://www.ietf.org/how/wgs/">working groups</a>, which are organized by topic into several areas (e.g., routing, transport, security, etc.). The IETF working groups are grouped into areas, and managed by Area Directors, or ADs. The ADs are members of the Internet Engineering Steering Group (<a href="https://www.ietf.org/glossary.html#IESG">IESG</a>).</p>
<p>Much of the work in the IETF is handled via <a href="https://www.ietf.org/list/">mailing lists</a>. The IETF holds <a href="https://www.ietf.org/how/meetings/">meetings</a> three times per year. <a href="https://www.ietf.org/how/runningcode/hackathons/">IETF Hackathons</a> encourage collaboration on developing utilities, ideas, sample code, and solutions that show practical implementations of IETF standards. More recently GitHub has become a prevalent way to track the work and interact with developers more smoothly; for example <a href="https://github.com/core-wg">CoRE</a>, <a href="https://github.com/httpwg">HTTP</a> or <a href="https://github.com/ietf-teep">TEEP</a> use it regularly.</p>
<p>The Internet Architecture Board, (<a href="https://www.ietf.org/glossary.html#IAB">IAB</a>). The IAB also adjudicates appeals when someone complains that the IESG has failed. The IAB and IESG are chartered by the Internet Society (<a href="https://www.ietf.org/glossary.html#ISOC">ISOC</a>) for these purposes. The General Area Director also serves as the chair of the IESG and of the IETF, and is an ex-officio member of the IAB.</p>
<p>There are very detailed guideline documents like <a href="https://tools.ietf.org/html/rfc2418">RFC2418</a> that explain how WGs are created and what the roles of each individual are, but at a high level the IETF relies on <strong>&quot;rough consensus and running code&quot;</strong>.</p>
<p>The technical work of the IETF is done in Working Groups, which are organized by topic into several Areas. Much of the work is handled via mailing lists. The IETF holds meetings three times per year.</p>
<p>The IETF working groups are grouped into areas, and managed by Area Directors, or ADs. The ADs are members of the Internet Engineering Steering Group (IESG). Providing architectural oversight is the Internet Architecture Board, (IAB). The IAB also adjudicates appeals when someone complains that the IESG has failed. The IAB and IESG are chartered by the Internet Society (ISOC) for these purposes. The General Area Director also serves as the chair of the IESG and of the IETF, and is an ex-officio member of the IAB. The Internet Assigned Numbers Authority (IANA) is the central coordinator for the assignment of unique parameter values for Internet protocols.</p>
<p>More information about the IETF standards process is available here and in RFC2026.</p>
<p>New participants in the IETF might find it helpful to read Getting Started in the IETF and The Tao of the IETF, (also available as RFC4677). You can learn more via tutorials or mentoring.</p>
<p><strong>IETF Working Groups (WGs)</strong> are the primary mechanism for the development of IETF specifications and guidelines, many of which are intended to be standards or recommendations.</p>
<p><strong>IRTF Research Group (RG)</strong> explore and work on research-related topics with a more longer-term approach when compared to a WG. However, its organizational nature remains similar.</p>
<a class="header" href="#the-life-of-a-draft" id="the-life-of-a-draft"><h2>The life of a draft</h2></a>
<p><strong>Internet-Drafts (I-Ds)</strong> are working documents of the IETF, its areas, and its Working Groups. Note that other groups may also distribute working documents as I-Ds. I-Ds can be proposed by individual parties, normally within the scope a specific WG. In order to become WG I-Ds they need to go through an adoption process within the WG itself.</p>
<p><strong>Request for Comment (RFC)</strong> is a technical and organizational publication, which describes mechanisms, implementations guidelines, or innovations applicable on topics related to Internet protocols, Internet-connected systems, applications, architecture and technology. In order to become RFC there needs to be several interoperable implementations, which often are Open Sourced.</p>
<p>RFCs can follow multiple tracks; standard, informational or experimental. Standard track RFCs that reach an adequate level of maturity become firstly <em>Proposed Standard</em>. Once that the proposed standard reaches a high degree of maturity and adoption can become <em>Internet Standard</em>.</p>
<a class="header" href="#contributing-to-ietf" id="contributing-to-ietf"><h2>Contributing to IETF</h2></a>
<p>Initially, the IETF met quarterly, but from 1991, it has been meeting three times a year. The initial meetings were very small, with fewer than 35 people in attendance at each of the first five meetings. The maximum attendance during the first 13 meetings was only 120 attendees. This occurred at the 12th meeting held during January 1989. These meetings have grown in both participation and scope a great deal since the early 1990s; it had a maximum attendance of 2,810 at the December 2000 IETF held in San Diego, California. Attendance declined with industry restructuring during the early 2000s, and is currently around 1,200.</p>
<a class="header" href="#communication-tools" id="communication-tools"><h3>Communication tools</h3></a>
<p><strong>mailing lists</strong>
<strong>jabber</strong>
<strong>interims</strong>
<strong>github</strong>
<strong>face to face</strong></p>
<a class="header" href="#references" id="references"><h2>References</h2></a>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLC86T-6ZTP5hXPJ-n4mwJbZ0BHaNlhTMA">IETF Newcomer's presentation</a></li>
<li><a href="https://www.ietf.org/about/participate/tutorials/newcomers/overview/">Newcomer's Overview</a></li>
<li><a href="./another10years.html">Another 10 years</a></li>
</ul>
<a class="header" href="#a-decade-of-internet-evolution" id="a-decade-of-internet-evolution"><h1>A decade of Internet Evolution</h1></a>
<p><em>This is an article published in the <a href="https://www.cisco.com/c/en/us/about/press/internet-protocol-journal/back-issues/table-contents-40/112-evolution.html">Internet Protocol Journal on June 2009 - Volume 11, number 2</a>. It was written by Vint Cerf, if the name does not ring a bell you are in the wrong field.</em></p>
<hr />
<p>In 1998 the Internet had about 50 million users, supported by approximately 25 million servers (Web and e-mail hosting sites, for example, but not desktops or laptops). In that same year, the Internet Corporation for Assigned Names and Numbers (ICANN) <a href="https://www.cisco.com/web/about/ac123/ac147/archived_issues/ipj_10-4/104_future.html">1</a> was created. Internet companies such as Netscape Communications, Yahoo!, eBay, and Amazon were already 3 to 4 years old and the Internet was in the middle of its so-called &quot;dot-boom&quot; period. Google emerged that year as a highly speculative effort to &quot;organize the world's information and make it accessible and useful.&quot; Investment in anything related to the Internet was called &quot;irrational exuberance&quot; by the then head of the U.S. Federal Reserve Bank, Alan Greenspan.</p>
<p>By April 2000, the Internet boom endedâat least in the United Statesâand a notable decline in investment in Internet application providers and infrastructure ensued. Domino effects resulted for router vendors, Internet service providers, and application providers. An underlying demand for Internet services remained, however, and it continued to grow, in part because of the growth in the number of Internet users worldwide.</p>
<p>During this same period, access to the Internet began to shift from dial-up speeds (on the order of kilobits to tens of kilobits per second) to broadband speeds (often measured in megabits per second). New access technologies such as digital subscriber loops and dedicated fiber raised consumer expectations of Internet capacity, in turn triggering much interest in streaming applications such as voice and video. In some locales, consumers could obtain gigabit access to the Internet (for example, in Japan and Stockholm). In addition, mobile access increased rapidly as mobile technology spread throughout the world, especially in regions where wireline telephony had been slow to develop.</p>
<p>Today the Internet has an estimated 542 million servers and about 1.3 billion users. Of the estimated 3 billion mobile phones in use, about 15 percent are Internet-enabled, adding 450 million devices to the Internet. In addition, at least 1 billion personal computers are in use, a significant fraction of which also have access to the Internet. The diversity of devices and access speeds on the Internet combine to produce challenges and opportunities for Internet application providers around the world. Highly variable speeds, display areas, and physical modes of interaction create a rich but complex canvas on which to develop new Internet applications and adapt older ones.</p>
<p>Another well-documented but unexpected development during this same decade is the dramatic increase in user-produced content on the Internet. There is no question that users contributed strongly to the utility of the Internet as the World Wide Web made its debut in the early 1990s with a rapidly growing menu of Web pages.</p>
<p>But higher speeds have encouraged user-produced audio and video archives (Napster and YouTube), as well as sharing of all forms of digital content through peer-to-peer protocols. Voice over IP, once a novelty, is very common, together with video conferencing (iChat from Apple, for example).</p>
<p>Geographically indexed information has also emerged as a major resource for Internet users. In the scientific realm, Google Earth and Google Maps are frequently used to display scientific data, sensor measurements, and so on. Local consumer information is another common theme. When I found myself in the small town of Page, Arizona, looking for saffron to make paella while in a houseboat on Lake Powell, a Google search on my Blackberry quickly identified markets in the area. I called one of them and verified that it had saffron in stock. I followed the map on the Website and bought 0.06 ounces of Spanish saffron for about $12.99. This experience reinforced my belief that having locally useful information at your fingertips no matter where you are is a powerful ally in daily living.</p>
<p>New business models based on the economics of digital information are also emerging. I can recall spending $1,000 for about 10 MB of disk storage in 1979. Recently I purchased 2 TB of disk storage for about $600. If I had tried to buy 2 TB of disk storage in 1979, it would have cost $200 million, and probably would have outstripped the production capacity of the supplier. The cost of processing, storing, and transporting digital information has changed the cost basis for businesses that once required the physical delivery of objects containing information (books, newspapers, magazines, CDs, and DVDs). The Internet can deliver this kind of information in digital form economicallyâand often more quickly than physical delivery. Older businesses whose business models are based on the costs of physical delivery of information must adapt to these new economics or they may find themselves losing business to online competitors. (It is interesting to note, however, that the Netflix business, which delivers DVDs by postal mail, has a respectable data rate of about 145 kbps per DVD, assuming a 3-day delivery time and about 4.7 GB per DVD. The CEO of Netflix, Reed Hastings, told me nearly 2 years ago that he was then shipping about 1.9 million DVDs per day, for an aggregate data rate of about 275 Gbps!)</p>
<p>Even the media that have traditionally been delivered electronically such as telephony, television, and radio are being changed by digital technology and the Internet. These media can now be delivered from countless sources to equally countless destinations over the Internet. It is common to think of these media as being delivered in streaming modes (that is, packets delivered in real time), but this need not be the case for material that has been prerecorded. Users of iPods have already discovered that they can download music faster than they can listen to it.</p>
<p>With gigabit access to the Internet, one could download an hour's worth of conventional video in about 16 seconds. This fact certainly changes my understanding of &quot;video on demand&quot; from a streaming delivery to a file transfer. The latter is much easier on the Internet because one is not concerned about packet inter-arrival times (jitter), loss, or even orderly delivery because the packets can be reordered and retransmitted during the file transfer. I am told that about 10 hours of video are being uploaded to YouTube per second.</p>
<p>The battles over Quality of Service (QoS) are probably not over yet either. Services such as Skype and applications such as iChat from Apple demonstrate the feasibility of credible, real-time audio and video conferencing on the &quot;best-efforts&quot; public Internet. I have been surprised by the quality that is possible when both parties have reasonably high-capacity access to the Internet.</p>
<p>Technorati is said to be tracking on the order of 112 million blogs, and the China Internet Network Information Center (CNNIC) estimates 72 million Chinese blogs that are probably in addition to those tracked by Technorati. Adding to these are billions of Web pages and, perhaps even more significant, an unknown amount of information online in the form of large databases. The latter are not indexed in the same way that Web pages can be, but probably contain more information. Think about high-energy physics information, images from the Hubble and other telescopes, radio telescope data including the Search for Extra-Terrestrial Intelligence (SETI) <a href="http://www.seti.org/">2</a>, and you quickly conclude that our modern society is awash in digital information.</p>
<p>It seems fair to ask how long accessibility of this information is likely to continue. By this question I do not mean that it may be lost from the Internet but, rather, that we may lose the ability to interpret it. I have already encountered such problems with image files whose formats are old and whose interpretation by newer software may not be possible. Similarly, I have ASCII text files from more than 20 years ago that I can still read, but I no longer have operating software that can interpret the formatting instructions to produce a nicely formatted page. I sometimes think of this problem as the &quot;year 3000&quot; problem: It is the year 3000 and I have just finished a Google search and found a PowerPoint 1997 file. Assuming I am running Windows 3000, it is a fair question whether the format of this file will still be interpretable. This problem would arise even if I were using open-source software. It seems unlikely that application software will last 1000 years in the normal course of events unless we deliberately take steps to preserve our ability to interpret digital content. Absent such actions, we will find ourselves awash in a sea of rotting bits whose meaning has long since been lost.</p>
<p>This problem is not trivial because questions will arise about intellectual property protection of the application, and even the operating system software involved. If a company goes out of business or asserts that it will no longer support a particular version of an application or operating system, do we need new regulations that require this software to be available on the public Internet in some way?</p>
<p>Even if we have skirted this problem in the past by rendering information into printed form, or microfilm, the complexity of digital objects is increasing. Consider spreadsheets or other complex objects that really cannot be fully &quot;rendered&quot; without the assistance of application software. So it will not be adequate simply to print or render information in other long-lived media formats. We really will need to preserve our ability to read and interpret bits.</p>
<p>The year 2008 also marks the tenth anniversary of a project that started at the U.S. Jet Propulsion Laboratory: The Interplanetary Internet. This effort began as a protocol design exercise to see what would have to change to make Internet-like capability available to manned and robotic spacecraft. The idea was to develop networking technology that would provide to the space exploration field the kind of rich and interoperable networking between spacecraft of any (Earth) origin that we enjoy between devices on the Internet.</p>
<p>The design team quickly recognized that the standard TCP/IP protocols would not overcome some of the long delays and disruptions to be expected in deep space communication. A new set of protocols evolved that could operate above the conventional Internet or on underlying transport protocols more suited to long delays and disruption. Called &quot;delay and disruption tolerant networking&quot;<a href="ftp://ftp.rfc-editor.org/in-notes/rfc4838.txt">3</a> or DTN, this suite of protocols is layered in the same abstract way as the Internet. The Interplanetary system could be thought of as a network of Internets, although it is not constrained to use conventional Internet protocols. The analog of IP is called the Bundle Protocol <a href="ftp://ftp.rfc-editor.org/in-notes/rfc5050.txt">4</a>, and this protocol can run above TCP or the User Datagram Protocol (UDP) or the new Licklider Transport Protocol (for deep space application). Ironically, the DTN protocol suite has also proven to be useful for terrestrial applications in which delay and disruption are common: tactical military communication and civilian mobile communication.</p>
<p>After 10 years of work, the DTN system will be tested onboard the Deep Impact mission platform late in 2008 as part of a program to qualify the new technology for use in future space missions. It is hoped that this protocol suite can be standardized for use by any of the world's space agencies so that spacecraft from any country will be interoperable with spacecraft of other countries and available to support new missions if they are still operational and have completed their primary missions. Such a situation already exists on Mars, where the Rovers are using previously launched orbital satellites to relay information to Earth's Deep Space Network using store-and-forward techniques like those common to the Internet.</p>
<p>The Internet has gone from dial-up to deep space in just the past 10 years. One can only begin to speculate about its application and condition 10 years hence. We will all have to keep our subscriptions to The Internet Protocol Journal to find out!</p>
<a class="header" href="#another-10-years-later" id="another-10-years-later"><h1>Another 10 Years Later</h1></a>
<p><em>This is an article published in the <a href="http://ipj.dreamhosters.com/wp-content/uploads/2018/08/ipj212.pdf">Internet Protocol Journal on August 2018 - Volume 21, number 2</a>. It was written by Geoff Huston who is Chief Scientist at APNIC, the Regional Internet Registry serving the Asia-Pacific region.</em></p>
<hr />
<p>The evolutionary path of any technology can often take strange and unanticipated turns and twists. At some points simplicity and minimalism can be replaced by complexity and ornamentation, while at other times a dramatic cut-through exposes the core concepts of the technology and removes layers of superfluous additions. The evolution of the Internet appears to be no exception and contains these same forms of unanticipated turns and twists. In thinking about the technology of the Internet over the last ten years, it appears that itâs been a very mixed story about whatâs changed and whatâs stayed the same.</p>
<p>A lot of the Internet today looks much the same as the Internet of a decade ago. Much of the Internetâs infrastructure has stubbornly resisted various efforts to engender change. We are still in the middle of the process to transition the Internet to IPv6, which was the case a decade ago. We are still trying to improve the resilience of the Internet to various attack vectors, which was the case a decade ago. We are still grappling with various efforts to provide defined quality of service in the network, which was the case a decade ago. It seems that the rapid pace of technical change in the 1990âs and early 2000âs has simply run out of momentum and it seems that the dominant activity on the Internet over the past decade was consolidation rather than continued technical evolution. Perhaps this increased resistance to change is because as the size of the network increases, its inertial mass also increases. We used to quote Metcalfâs Law to each other, reciting the mantra that the value of a network increases in proportion to the square of the number of users. A related observation appears to be that a networkâs inherent resistance to change, or inertial mass, is also directly related to the square of the number of users as well. Perhaps as a general observation, all large loosely coupled distributed systems are strongly resistant to efforts to orchestrate a coordinated change. At best, these systems respond to various forms of market pressures, but as the Internetâs overall system is so large and so diverse these market pressures manifest themselves in different ways in different parts of this network. Individual actors operate under no centrally orchestrated set of instructions or constraints. Where change occurs, it is because some sufficiently large body of individual actors see opportunity in undertaking the change or perceive unacceptable risk in not changing. The result for the Internet appears to be that some changes are very challenging, while others look like natural and inevitable progressive steps.</p>
<p>But the other side of the story is one that is about as diametrically opposed as its possible to paint. Over the last decade weâve seen another profound revolution in the Internet as it embraced a combination of wireless-based infrastructure and a rich set of services at a speed which has been unprecedented. Weâve seen a revolution in content and content provision that has not only changed the Internet, but as collateral damage the Internet appears to be decimating the traditional newspaper and broadcast television sectors. Social media has all but replaced the social role of the telephone and the practice of letter writing. Weâve seen the rise of the resurgence of a novel twist to the old central mainframe service in the guise of the âcloudâ and the repurposing of Internet devices to support views of a common cloud-hosted content that in many ways mimic the function of display terminals of a bygone past. All of these are fundamental changes to the Internet and all of these have occurred in the last decade!</p>
<p>Thatâs a significant breadth of material to cover, so Iâll keep the story to the larger themes, and to structure this story, rather than offer a set of unordered observations about the various changes and developments over the past decade, Iâll use a standard model of a protocol stack as the guiding template. Iâll start with the underlying transmission media and then looking at IP, the transport layer, then applications and services, and closing with a look at the business of the Internet to highlight the last decadeâs developments.</p>
<hr />
<a class="header" href="#below-the-ip-layer" id="below-the-ip-layer"><h2>Below the IP Layer</h2></a>
<p>Whatâs changed in network media?</p>
<p>Optical systems have undergone sustained change in the past decade. A little over a decade ago production optical systems used simple on-off keying to encode the signal into the optical channel. The speed increases in this generation of optical systems relied on improvements in the silicon control systems and the laser driver chips. The introduction of wavelength division multiplexing in the late 1990âs allowed the carriers to greatly increase the carrying capacity of their optical cable infrastructure. The last decade has seen the evolution of optical systems into areas of polarisation and phase modulation to effectively lift the number of bits of signal per baud. These days 100Gbps optical channels are commonly supportable, and we are looking at further refinements in signal detection to lift that beyond 200Gbps. We anticipate 400Gbps systems in the near future, using various combinations of a faster basic baud rate and higher levels of phase amplitude modulation, and dare to think that 1Tbps is now a distinct near term optical service.</p>
<p>Radio systems have seen a similar evolution in overall capacity. Basic improvements in signal processing, analogous to the changes in optical systems, has allowed the use of phase modulation to lift the data rate of the radio bearer. The use of MIMO technology, coupled with the use of higher carrier frequencies has allowed the mobile data service to support carriage services of up to 100Mbps in todayâs 4G networks. The push to even higher frequencies promises speeds of up to 1Gbps for mobile systems in the near future with the deployment of 5G technology.</p>
<p>While optical speeds are increasing, ethernet packet framing still persists in transmission systems long after the original rationale for the packet format died along with that bright yellow coaxial cable! Oddly enough, the Ethernet-defined minimum and maximum packet sizes of 64 and 1500 octets still persist. The inevitable result of faster transmission speeds with constant packet sizes results in an upper bound of the number of packets per second increasing more 100-fold over the past decade, in line with the increase of deployed transmission speeds from 2.5Gbps to 400 Gbps. As a consequence, higher packet processing rates are being demanded from silicon-based switches. But one really important scaling factor has not changed for the past decade, namely the clock speed of processors and the cycle time of memory, which has not moved at all. The response so far has been in increasing reliance of parallelism in high speed digital switching applications, and these days multi-core processors and highly parallel memory systems are used to achieve performance that would be impossible in a single threaded processing model.</p>
<p>In 2018 it appears that we are close to achieving 1Tbps optical systems and up to 20Gbps in radio systems. Just how far and how quickly these transmission models can be pushed into supporting ever higher channel speeds is an open question.</p>
<hr />
<a class="header" href="#the-ip-layer" id="the-ip-layer"><h2>The IP Layer</h2></a>
<p>The most notable aspect of the network that appears to stubbornly resist all forms of pressure over the last decade, including some harsh realities of acute scarcity, is the observation that we are still running what is essentially an IPv4 Internet.</p>
<p>Over this past decade we have exhausted our pools of remaining IPv4 addresses, and in most parts of the world the IPv4 Internet is running on some form of empty. We had never suspected that the Internet would confront the exhaustion of one its most fundamental pillars, the basic function of uniquely addressing connected devices, and apparently shrug it off and continue on blithely. But, unexpectedly, thatâs exactly whatâs happened.</p>
<p>Today we estimate that some 3.4 billion people are regular users of the Internet, and there are some 20 billion devices connected to it. We have achieved this using some 3 billion unique IPv4 addresses. Nobody thought that we could achieve this astonishing feat, yet it has happened with almost no fanfare.</p>
<p>Back in the 1900âs we had thought that the prospect of address exhaustion would propel the Internet to use IPv6. This was the successor IP protocol that comes with a four-fold increase in the bit width of IP addresses. By increasing the IP address pool to some esoterically large number of unique addresses (340 undecillion addresses, or 3.4 x 1038) we would never have to confront network address exhaustion again. But this was not going to be an easy transition. There is no backward compatibility in this protocol transition, so everything has to change. Every device, every router and even every application needs to change to support IPv6. Rather than perform comprehensive protocol surgery on the Internet and change every part of the infrastructure to support IPv6, we changed the basic architecture of the Internet instead. Oddly enough, it looks like this was the cheaper option!</p>
<p>Through the almost ubiquitous deployment of Network Address Translators (NATs) at the edges of the network, weâve transformed the network from a peer-to-peer network into a client/server network. In todayâs client/server Internet clients can talk to servers, and servers can talk back to these connected clients, but thatâs it. Clients cannot talk directly to other clients, and servers need to wait for the client to initiate a conversation in order to talk to a client. Clients âborrowâ an endpoint address when they are talking to a server and release this address for use by other clients when they are idle. After all, endpoint addresses are only useful to clients in order to talk to servers. The result is that weâve managed to cram some 20 billion devices into an Internet that only has deployed just 3 billion public address slots. Weâve achieved this though embracing what could be described as time-sharing of IP addresses.</p>
<p>All well and good, but what about IPv6? Do we still need it? If so, then then are we going to complete this protracted transition? Ten years later the answer to these questions remain unclear. On the positive side, there is a lot more IPv6 around now than there was ten years ago. Service Providers are deploying much IPv6 today than was the case in 2008. When IPv6 is deployed within a Service Providerâs network we see an immediate uptake from these IPv6-equipped devices. In 2018 it appears that one fifth of the Internetâs users (that itself is now estimated to number around one half of the planetâs human population) are capable of using the Internet over IPv6, and most of this has happened in the past 10 years. However, on the negative side the question must be asked: Whatâs happening with IPv6 for the other four fifths of the Internet? Some ISPs have been heard to make the case that they would prefer to spend their finite operating budgets on other areas that improve their customersâ experience such as increasing network capacity, removing data caps, acquiring more on-net content. Such ISPs continue to see deployment of IPv6 as a deferable measure.</p>
<p>It seems that today we are still seeing a mixed picture for IPv6. Some service providers simply see no way around their particular predicament of IPv4 address scarcity and these providers see IPv6 as a necessary decision to further expand their network. Other providers are willing to defer the question to some undefined point in the future.</p>
<a class="header" href="#routing" id="routing"><h3>Routing</h3></a>
<p>While we are looking at whatâs largely unchanged over the past decade we need to mention the routing system. Despite dire predictions of the imminent scaling death of the Border Gateway Protocol (BGP) ten years ago, BGP has steadfastly continued to route the entire Internet. Yes, BGP is as insecure as ever, and yes, a continual stream of fat finger foul-ups and less common but more concerning malicious route hijacks continue to plague our routing system, but the routing technologies in use in 2008 are the same as we use in todayâs Internet.</p>
<p>The size of the IPv4 routing table has tripled in the past ten years, growing from 250,000 entries in 2008 to slightly more than 750,000 entries today. The IPv6 routing story is more dramatic, growing from 1,100 entries to 52,000 entries. Yet BGP just quietly continues to work efficiently and effectively. Who wouldâve thought that a protocol that was originally designed to cope with a few thousand routes announced by a few hundred networks could still function effectively across a routing space approaching a million routing entries and a hundred thousand networks!</p>
<p>In the same vein, we have not made any major change to the operation of our interior routing protocols. Larger networks still use either OPSF or ISIS depending on their circumstances, while smaller networks may opt for some distance vector protocol like RIPv2 or even EIGRP. The work in the IETF on more recent routing protocols LISP and BABEL seem lack any real traction with the Internet at large, and while they both have interesting properties in routing management, neither have a sufficient level of perceived benefit to overcome the considerable inertia of conventional network design and operation. Again, this looks like another instance where inertial mass is exerting its influence to resist change in the network.</p>
<a class="header" href="#network-operations" id="network-operations"><h3>Network Operations</h3></a>
<p>Speaking of network operation, we are seeing some stirrings of change, but it appears to be a rather conservative area, and adoption of new network management tools and practices takes time.</p>
<p>The Internet converged on using the Simple Network Management Protocol (SNMP) a quarter of a century ago, and despite its security weaknesses, its inefficiency, its incredibly irritating use of ASN.1, and its use in sustaining some forms of DDOS attacks, it still enjoys widespread use. But SNMP is only a network monitoring protocol, not a network configuration protocol, as anyone who has attempted to use SNMP write operations can attest.</p>
<p>The more recent Netconf and YANG efforts are attempting to pull this area of configuration management into something a little more usable than expect scripts driving CLI interfaces on switches. At the same time, we are seeing orchestration tools such as Ansible, Chef, NAPALM and SALT enter the network operations space, permitting the orchestration of management tasks over thousands of individual components. These network operations management tools are welcome steps forward to improve the state of automated network management, but itâs still far short of a desirable endpoint.</p>
<p>In the same time period as we appear to have advanced the state of automated control systems to achieve the driverless autonomous car, the task of fully automated network management appears to have fallen way short of the desired endpoint. Surely it must be feasible to feed an adaptive autonomous control system with the networkâs infrastructure and available resources, and allow the control system to monitor the network and modify the operating parameters of network components to continuously meet the networkâs service level objectives? Whereâs the driverless car for driving networks? Maybe the next ten years might get us there.</p>
<a class="header" href="#the-mobile-internet" id="the-mobile-internet"><h3>The Mobile Internet</h3></a>
<p>Before we move up a layer in the Internet protocol model and look at the evolution of the end-to-end transport layer, we probably need to talk about the evolution of the devices that connect to the Internet.</p>
<p>For many years the Internet was the domain of the desktop personal computer, with laptop devices serving the needs to those with a desire for a more portable device. At the time the phone was still just a phone, and their early forays into the data world were unimpressive.</p>
<p>Appleâs iPhone, released in 2007, was a revolutionary device. Boasting a vibrant colour touch sensitive screen, just four keys, a fully functional operating system, with WiFi and cellular radio interfaces, and a capable processor and memory, itâs entry into the consumer market space was perhaps the major event of the decade. Appleâs early lead was rapidly emulated by Windows and Nokia with their own offerings. Googleâs position was more as an active disruptor, using an open licensing framework for the Android platform and its associated application ecosystem to empower a collection of handset assemblers. Android is used by Samsung, LG, HTC, Huawei, Sony, and Google to name a few. These days almost 80% of the mobile platforms use Android, and some 17% use Appleâs iOS.</p>
<p>For the human Internet the mobile market is now the Internet-defining market in terms of revenue. There is little in terms of margin or opportunity in the wired network these days, and even the declining margins of these mobile data environments represent a vague glimmer of hope for the one dominant access provider industry.</p>
<p>Essentially, the public Internet is now a platform of apps on mobile devices.</p>
<hr />
<a class="header" href="#end-to-end-transport-layer" id="end-to-end-transport-layer"><h2>End to End Transport Layer</h2></a>
<p>Itâs time to move up a level in the protocol stack and look at end-to-end transport protocols and changes that have occurred in the past decade.</p>
<p>End-to-end transport was the revolutionary aspect of the Internet, and the TCP protocol was at the heart of this change. Many other transport protocols require the lower levels of the network protocol stack to present a reliable stream interface to the transport protocol. It was up to the network to create this reliability, performing data integrity checks and data flow control, and repairing data loss within the network as it occurred. TCP dispensed with all of that, and simply assumed an unreliable datagram transport service from the network and pushed to the transport protocol the responsibility for data integrity and flow control.</p>
<p>In the world of TCP not much appears to have changed in the past decade. Weâve seen some further small refinements in the details of TCPâs controlled rate increase and rapid rate decrease, but nothing that shifts the basic behaviours this protocol. TCP tends to use packet loss as the signal of congestion and oscillates its flow rate between some lower rate and this loss-triggering rate.</p>
<p>Or at least that was the case until quite recently. The situation is poised to change, and change in a very fundamental way, with the debut of Googleâs offerings of BBR and QUIC.</p>
<p>The Bottleneck Bounded Rate control algorithm, or BBR, is a variant of the TCP flow control protocol that operates in a very different mode from other TCP protocols. BBR attempts to maintain a flow rate that sits exactly at the delay bandwidth product of the end-to-end path between sender and receiver. In so doing, tries to avoid the accumulation of data buffering in the network (when the sending rate exceeds the path capacity), and also tries to avoid leaving idle time in the network (where the sending rate is less than the path capacity). The side effect is that BBR tries to avoid the collapse of network buffering when congestion-based loss occurs. BBR achieves significant efficiencies from both wired and wireless network transmission systems.</p>
<p>The second recent offering from Google also represents a significant shift in the way we use transport protocols. The QUIC protocol looks like a UDP protocol, and from the networkâs perspective it is simply a UDP packet stream. But in this case looks are deceiving. The inner payload of these UDP packets contain a more conventional TCP flow control structure and a TCP stream payload. However, QUIC encrypts its UDP payload so the entire inner TCP control is completely hidden from the network. The ossification of the Internetâs transport is due in no small part to the intrusive role of network middleware that is used to discarding packets that it does not recognise. Approaches such as QUIC allow applications to break out of this regime and restore end-to-end flow management as an end-to-end function without any form of network middleware inspection or manipulation. Iâd call this development as perhaps the most significant evolutionary step in transport protocols over the entire decade.</p>
<hr />
<a class="header" href="#the-application-layer" id="the-application-layer"><h2>The Application Layer</h2></a>
<p>Letâs keep on moving up the protocol stack and look at the Internet from the perspective of the applications and services that operate across the network.</p>
<a class="header" href="#privacy-and-encryption" id="privacy-and-encryption"><h3>Privacy and Encryption</h3></a>
<p>As we noted in looking at developments in end-to-end transport protocols, encryption of the QUIC payload is not just to keep network middleware from meddling with the TCP control state, although it does achieve that very successfully. The encryption applies to the entire payload, and it points to another major development in the past decade. We are now wary of the extent to which various forms of network-based mechanisms are used to eavesdrop on users and services. The documents released by Edward Snowden in 2013 portrayed a very active US Government surveillance program that used widespread traffic interception sources to construct profiles of user behaviour and by inference profiles of individual users. In many ways this effort to assemble such profiles is not much different to what advertising-funded services such as Google and Facebook have been (more or less) openly doing for years, but perhaps the essential difference is that of knowledge and implied consent. In the advertisersâ case this information is intended to increase the profile accuracy and hence increase the value of the user to the potential advertiser. The motivations of government agencies are more open to various forms of interpretation, and not all such interpretations are benign.</p>
<p>One technical response to the implications of this leaked material has been an overt push to embrace end-to-end encryption in all parts of the network. The corollary has been an effort to allow robust encryption to be generally accessible to all, and not just a luxury feature available only to those who can afford to pay a premium. The Letâs Encrypt initiative has been incredibly successful in publishing X.509 domain name certificates that free of cost, and the result is that all network service operators, irrespective of their size or relative wealth, can afford to use encrypted sessions, in the form of TLS, for their web servers.</p>
<p>The push to hide user traffic from the network and network-based eavesdroppers extends far beyond QUIC and TLS session protocols. The Domain Name System is also a rich source of information about what users are doing, as well as being used in many places to enforce content restrictions. There have been recent moves to try and clean up the overly chatty nature of the DNS, using query name minimisation to prevent unnecessary data leaks, and the development of both DNS over TLS and DNS over HTTPS to secure the network path between a stub resolver and its recursive server. This is very much a work in progress effort at present, and it will take some time to see if the results of this work will be widely adopted in the DNS environment.</p>
<p>We are now operating our applications in an environment of heightened paranoia. Applications do not necessarily trust the platform on which they are running, and we are seeing efforts from the applications to hide their activity from the underlying platform. Applications do not trust the network, and we are seeing increased use of end-to-end encryption to hide their activity from network eaves droppers. The use of identity credentials within the encrypted session establishment also acts to limit the vulnerability of application clients to be misdirected to masquerading servers.</p>
<p>###Â The Rise and Rise of Content</p>
<p>Moving further up the protocol stack to the environment of content and applications we have also seen some revolutionary changes over the past decade.</p>
<p>For a small period of time the Internetâs content and carriage activities existed in largely separate business domains, tied by mutual interdependence. The task of carriage was to carry users to content, which implied that carriage was essential to content. But at the same time a client/server Internet bereft of servers is useless, so content is essential to carriage. In a world of re-emerging corporate behemoths, such mutual interdependence is unsettling, both to the actors directly involved, and to the larger public interest.</p>
<p>The content industry is largely the more lucrative of these two and enjoys far less in the way of regulatory constraint. There is no concept of any universal service obligation, or even any effective form of price control in the services they offer. Many content service providers use internal cross funding that allows them to offer free services to the public, as in free email, free content hosting, free storage, and similar, and fund these services through a second, more occluded, transaction that essentially sells the userâs consumer profile to the highest bidding advertiser. All this happens outside of any significant regulatory constraint which has given the content services industry both considerable wealth and considerable commercial latitude.</p>
<p>It should be no surprise that this industry is now using its capability and capital to eliminate its former dependence on the carriage sector. We are now seeing the rapid rise of the content data network (CDN) model, where instead of an Internet carrying the user to a diverse set of content stores, the content stores are opening local content outlets right next to the user. As all forms of digital services move into CDN hostels, and as the CDN opens outlets that are positioned immediately adjacent to pools of economically valuable consumers, then where does that leave the traditional carriage role in the Internet? The outlook for the public carriage providers is not looking all that rosy given this increasing marginalisation of carriage in the larger content economy.</p>
<p>Within these CDNs weâve also seen the rise of a new service model enter the Internet in the form of cloud services. Our computers are no longer self-contained systems with processing and compute resources but look more and more like a window that sees the data stored on a common server. Cloud services are very similar, where the local device is effectively a local cache of a larger backing store. In a world where users may have multiple devices this model makes persuasive sense, as the view to the common backing store is constant irrespective of which device is being used to access the data. These cloud services also make data sharing and collaborative work far easier to support. Rather than creating a set of copies of the original document and then attempt to stitch back all the individual edits into a single common whole, the cloud model shares a document by simply altering the documentâs access permissions. There is only ever one copy of the document, and all edits and comments on the document are available to all.</p>
<a class="header" href="#the-evolution-of-cyber-attacks" id="the-evolution-of-cyber-attacks"><h3>The Evolution of Cyber Attacks</h3></a>
<p>At the same time as we have seen announcements of ever increasing network capacity within the Internet weâve seen a parallel set of announcements that note new records in the aggregate capacity of Denial of Service attacks. The current peak volume is an attack of some 1.7Tbps of malicious traffic.</p>
<p>Attacks are now commonplace. Many of them are brutally simple, relying on a tragically large pool of potential zombie devices that are readily subverted and co-opted to assist in attacks. The attacks are often simple forms of attack, such as UDP reflection attacks where a simple UDP query generates a large response. The source address of the query is forged to be the address of the intended attack victim, and not much more need be done. A small query stream can result in a massive attack. UDP protocols such as SNMP, NTP, the DNS and memcache have been used in the past and doubtless will be used again.</p>
<p>Why canât we fix this? Weâve been trying for decades, and we just canât seem to get ahead of the attacks. Advice to network operators to prevent the leakage of packets with forged source addresses, RFC 2827, was published in two decades ago in 1998. Yet massive UDP-based attacks with forged source addresses persist all the way through today. Aged computer systems with known vulnerabilities continued to be connected to the Internet and are readily transformed into attack bots.</p>
<p>The picture of attacks is also becoming more ominous. Previously attributed to âhackersâ it was quickly realised that a significant component of these hostile attacks had criminal motivations. The progression from criminal actors to state-based actors is also entirely predictable, and we are seeing an escalation of this cyber warfare arena with the investment in various forms of exploitation of vulnerabilities being seen as part of a set of desirable national capabilities.</p>
<p>It appears that a major problem here is that collectively we are unwilling to make any substantial investment in effective defence or deterrence. The systems that we use on the Internet are overly trusting to the point of irrational credulity. For example, the public key certification system used to secure web-based transactions is repeatedly demonstrated to be entirety untrustworthy, yet thatâs all we trust. Personal data is continually breached and leaked, yet all we seem to want to do is increase the number and complexity of regulations rather than actually use better tools that would effectively protect users.</p>
<p>The larger picture of hostile attack is not getting any better. Indeed, itâs getting very much worse. If any enterprise has a business need to maintain a service that is always available for use, then any form of in-house provisioning is just not enough to be able to withstand attack. These days only a handful of platforms are able to offer resilient services, and even then itâs unclear whether they could withstand the most extreme of attacks. There is a constant background level of scanning and probing going on in the network, and any form of visible vulnerability is ruthlessly exploited. One could describe todayâs Internet as a toxic wasteland, punctuated with the occasional heavily defended citadel. Those who can afford to locate their services within these citadels enjoy some level of respite from this constant profile of hostile attack, while all others are forced to try and conceal themselves from the worst of this toxic environment, while at the same time aware that they will be completely overwhelmed by any large scale attack.</p>
<p>Itâs a sobering through that about one half of the worldâs population are now part of this digital environment. A more sobering thought is that many of todayâs control systems, such as power generation and distribution, water distribution, and road traffic control systems are exposed to the Internet. Perhaps even more of a worry is the increasing use of the Internet in automated systems that include various life support functions. The consequences of massive failure of these systems in the face of a sustained and damaging attack cannot be easily imagined.</p>
<a class="header" href="#the-internet-of-billions-of-tragically-stupid-things" id="the-internet-of-billions-of-tragically-stupid-things"><h3>The Internet of Billions of Tragically Stupid Things</h3></a>
<p>What makes this scenario even more depressing is the portent of the so-called Internet of Things.</p>
<p>In those circles where Internet prognostications abound and policy makers flock to hear grand visions of the future, we often hear about the boundless future represented by this âInternet of Things. This phrase encompasses some decades of the computing industryâs transition from computers as esoteric pieces of engineering affordable only by nations, to mainframes, desktops, laptops, handhelds, and now wrist computers. Where next? In the vision of the Internet of Things we are going to expand the Internet beyond people and press on with using billions of these chattering devices in every aspect of our world.</p>
<p>What do we know about the âthingsâ that are already connected to the Internet?</p>
<p>Some of them are not very good. In fact some of them are just plain stupid. And this stupidity is toxic, in that their sometime inadequate models of operation and security affects others in potentially malicious ways. Doubtless if such devices were constantly inspected and managed we might see evidence of aberrant behaviour and correct it. But these are unmanaged devices that are all but invisible. There are the controller for a web camera, the so-called âsmartâ thin in a smart television, or what controls anything from a washing machine to a goods locomotive. Nobody is looking after these devices.</p>
<p>When we think of an Internet of Things we think of a world of weather stations, web cams, âsmartâ cars, personal fitness monitors and similar. But what we tend to forget is that all of these devices are built upon layers of other peopleâs software that is assembled into a product at the cheapest possible price point. It may be disconcerting to realise that the web camera you just installed has a security model that can be summarised with the phrase: âno security at allâ, and its actually offering a view of your house to the entire Internet. It may be slightly more disconcerting to realise that your electronic wallet is on a device that is using a massive compilation of open source software of largely unknown origin, with a security model that is not completely understood, but appears to be susceptible to be coerced into being a âyes, take all you wantâ.</p>
<p>It would be nice to think that weâve stopped making mistakes in code, and from now on our software in our things will be perfect. But thatâs hopelessly idealistic. Itâs just not going to happen. Software will not be perfect. It will continue to have vulnerabilities. It would be nice to think that this Internet of Things is shaping up as a market where quality matters, and consumers will select a more expensive product even though its functional behaviour is identical to a cheaper product that has not been robustly tested for basic security flaws. But that too is hopelessly naive.</p>
<p>The Internet of Things will continue to be a market place where the compromises between price and quality will continue to push us on to the side of cheap rather than secure. Whatâs going to stop us from further polluting our environment with a huge and diverse collection of programmed unmanaged devices with inbuilt vulnerabilities that will be all too readily exploited? What can we do to make this world of these stupid cheap toxic things less stupid and less toxic? Workable answers to this question have not been found so far.</p>
<hr />
<a class="header" href="#the-next-ten-years" id="the-next-ten-years"><h2>The Next Ten Years</h2></a>
<p>The silicon industry is not going to shut down anytime soon. It will continue to produce chips with more gates, finer tracks and more stacked layers for some years to come. Our computers will become more capable in terms of the rage and complexity of the tasks that they will be able to undertake.</p>
<p>At the same time, we can expect more from our network. Higher capacity certainly, but also greater levels of customisation of the network to our individual needs.</p>
<p>However, I find it extremely challenging to be optimistic about security and trust in the Internet. We have made little progress in this areas over the last ten years and there is little reason to think that the picture will change in the next ten years. If we canât fix it, then, sad as it sounds, perhaps we simply need to come to terms with an Internet jammed full of tragically stupid things.</p>
<p>However, beyond these broad-brush scenarios, itâs hard to predict where the Internet will head. Technology does not follow a pre-determined path. Itâs driven by the vagaries of an enthusiastic consumer market place that is readily distracted by colourful bright shiny new objects, and easily bored by what we quickly regard as commonplace.</p>
<p>What can we expect from the Internet in the next ten years that can outdo a pocket-sized computer that can converse with me in a natural language? That can offer more than immersive 3D video in outstanding quality? That can bring the entire corpus of humanityâs written work into a searchable database that can answer any of our questions in mere fractions of a second?</p>
<p>Personally, I have no clue what to expect from the Internet. But whatever does manage to capture our collective attention I am pretty confident that it will be colourful, bright, shiny, and entirely unexpected!</p>
<a class="header" href="#ddos-using-coap" id="ddos-using-coap"><h1>DDOS using CoAP</h1></a>
<a class="header" href="#overview-of-network-attacks-on-the-internet" id="overview-of-network-attacks-on-the-internet"><h2>Overview of Network Attacks on the Internet</h2></a>
<p>(Refer to another10years for users and for attacks)
(Refer to another10years for iotattacks)</p>
<p>Here are some of my notes about the event, what are the causes and how can it be prevented.</p>
<a class="header" href="#udp-based-attacks" id="udp-based-attacks"><h2>UDP-based Attacks</h2></a>
<a class="header" href="#reflection-attack" id="reflection-attack"><h3>Reflection Attack</h3></a>
<p>asdasdasd</p>
<a class="header" href="#amplification-attack" id="amplification-attack"><h3>Amplification Attack</h3></a>
<p>asdasdasd</p>
<a class="header" href="#past-attacks-using-udp" id="past-attacks-using-udp"><h3>Past attacks using UDP</h3></a>
<p>NTP, SNMP, DNS
##Â Enter the Constrained Application Protocol</p>
<a class="header" href="#attacks-using-coap-and-udp" id="attacks-using-coap-and-udp"><h2>Attacks using CoAP and UDP</h2></a>
<a class="header" href="#solutions" id="solutions"><h2>Solutions</h2></a>
<a class="header" href="#references-1" id="references-1"><h2>References</h2></a>
<ul>
<li>RFC2827</li>
<li><a href="./another10years.html">Another 10 years</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
